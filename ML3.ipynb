{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "* Hospital/bank/customer customer service - how many males/females are calling\n",
    "* \n",
    "\n",
    "### Streaming\n",
    "* Yes, with the proper data preprocessing it would be possible. We only need to extract the chosen features. We would probably have to extract only the last small part (maybe 1 second) the data in order to identify the gender of the person currently talking. However this would probably only work with one person talking at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://raw.githubusercontent.com/OnkelDunkel/MLAssignment2/master/voice.csv\"\n",
    "df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12206239ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.corr(),linewidths=0.50,vmax=1.0, square=True, cmap=\"Blues\", linecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we see correlations in feature space?\n",
    "* SD (standard deviation of frequency), IQR, skew, kurt, spectral entropy (sp.ent), sectral flatness (sfm) and modinx (modulation index) are statistical numbers. As seen in the correlation matrix they have little correlation with all other features in the dataset. This means that we might be able to remove those feature to increase performance and maybe achieve a better classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meanfreq    False\n",
       "sd          False\n",
       "median      False\n",
       "Q25         False\n",
       "Q75         False\n",
       "IQR         False\n",
       "skew        False\n",
       "kurt        False\n",
       "sp.ent      False\n",
       "sfm         False\n",
       "mode        False\n",
       "centroid    False\n",
       "meanfun     False\n",
       "minfun      False\n",
       "maxfun      False\n",
       "meandom     False\n",
       "mindom      False\n",
       "maxdom      False\n",
       "dfrange     False\n",
       "modindx     False\n",
       "label       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    1584\n",
       "male      1584\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1584\n",
       "0    1584\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label = np.where(df.label.values == 'female', 1, 0)\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['meanfreq', 'sd', 'median', 'Q25', 'Q75', 'IQR', 'skew', 'kurt',\n",
       "       'sp.ent', 'sfm', 'mode', 'centroid', 'meanfun', 'minfun', 'maxfun',\n",
       "       'meandom', 'mindom', 'maxdom', 'dfrange', 'modindx', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = df.drop('label', axis = 1)\n",
    "#y = df.label\n",
    "#x = df.iloc[:,0:20]\n",
    "#y = df.iloc[:,-1].va\n",
    "#y= y.astype('float32')\n",
    "#x= x.astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.label.values\n",
    "x = df.drop([\"label\"], axis = 1)\n",
    "x = (x - x.min())/(x.max() - x.min()) # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>mode</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.473409</td>\n",
       "      <td>0.084125</td>\n",
       "      <td>0.060063</td>\n",
       "      <td>0.204956</td>\n",
       "      <td>0.254828</td>\n",
       "      <td>0.367853</td>\n",
       "      <td>0.208279</td>\n",
       "      <td>0.635798</td>\n",
       "      <td>0.564526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.157706</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.505075</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.077635</td>\n",
       "      <td>0.215683</td>\n",
       "      <td>0.246961</td>\n",
       "      <td>0.644279</td>\n",
       "      <td>0.483766</td>\n",
       "      <td>0.630964</td>\n",
       "      <td>0.591578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125828</td>\n",
       "      <td>0.287642</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.056449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179222</td>\n",
       "      <td>0.675536</td>\n",
       "      <td>0.102873</td>\n",
       "      <td>0.034284</td>\n",
       "      <td>0.385912</td>\n",
       "      <td>0.457148</td>\n",
       "      <td>0.885255</td>\n",
       "      <td>0.782275</td>\n",
       "      <td>0.442738</td>\n",
       "      <td>0.548382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179222</td>\n",
       "      <td>0.236945</td>\n",
       "      <td>0.030264</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.049885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528261</td>\n",
       "      <td>0.554611</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>0.389906</td>\n",
       "      <td>0.715802</td>\n",
       "      <td>0.407358</td>\n",
       "      <td>0.031549</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.856457</td>\n",
       "      <td>0.299565</td>\n",
       "      <td>0.528261</td>\n",
       "      <td>0.183442</td>\n",
       "      <td>0.041287</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.065659</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.265043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.452195</td>\n",
       "      <td>0.627209</td>\n",
       "      <td>0.454272</td>\n",
       "      <td>0.317627</td>\n",
       "      <td>0.707515</td>\n",
       "      <td>0.474474</td>\n",
       "      <td>0.027742</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.958736</td>\n",
       "      <td>0.926348</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>0.452195</td>\n",
       "      <td>0.279190</td>\n",
       "      <td>0.036829</td>\n",
       "      <td>0.929285</td>\n",
       "      <td>0.238994</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.250536</td>\n",
       "      <td>0.250715</td>\n",
       "      <td>0.223380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.441173</td>\n",
       "      <td>0.631448</td>\n",
       "      <td>0.432029</td>\n",
       "      <td>0.274076</td>\n",
       "      <td>0.722901</td>\n",
       "      <td>0.534679</td>\n",
       "      <td>0.051782</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.922681</td>\n",
       "      <td>0.870197</td>\n",
       "      <td>0.401984</td>\n",
       "      <td>0.441173</td>\n",
       "      <td>0.299699</td>\n",
       "      <td>0.037761</td>\n",
       "      <td>0.857144</td>\n",
       "      <td>0.098448</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.124464</td>\n",
       "      <td>0.134238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.526061</td>\n",
       "      <td>0.578887</td>\n",
       "      <td>0.595932</td>\n",
       "      <td>0.375003</td>\n",
       "      <td>0.706098</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.940728</td>\n",
       "      <td>0.900382</td>\n",
       "      <td>0.307846</td>\n",
       "      <td>0.526061</td>\n",
       "      <td>0.276701</td>\n",
       "      <td>0.084682</td>\n",
       "      <td>0.929285</td>\n",
       "      <td>0.159942</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.242673</td>\n",
       "      <td>0.242847</td>\n",
       "      <td>0.132985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.572113</td>\n",
       "      <td>0.602659</td>\n",
       "      <td>0.532916</td>\n",
       "      <td>0.446359</td>\n",
       "      <td>0.819942</td>\n",
       "      <td>0.449670</td>\n",
       "      <td>0.036301</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.906544</td>\n",
       "      <td>0.847309</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.572113</td>\n",
       "      <td>0.205893</td>\n",
       "      <td>0.041084</td>\n",
       "      <td>0.233218</td>\n",
       "      <td>0.099505</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.024303</td>\n",
       "      <td>0.024320</td>\n",
       "      <td>0.304531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.485814</td>\n",
       "      <td>0.615573</td>\n",
       "      <td>0.509942</td>\n",
       "      <td>0.356014</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>0.445258</td>\n",
       "      <td>0.027701</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.953672</td>\n",
       "      <td>0.910746</td>\n",
       "      <td>0.782511</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>0.226085</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.111416</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.098642</td>\n",
       "      <td>0.098712</td>\n",
       "      <td>0.159026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.448457</td>\n",
       "      <td>0.639632</td>\n",
       "      <td>0.441466</td>\n",
       "      <td>0.304920</td>\n",
       "      <td>0.689783</td>\n",
       "      <td>0.470487</td>\n",
       "      <td>0.030322</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.972260</td>\n",
       "      <td>0.952323</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.448457</td>\n",
       "      <td>0.276351</td>\n",
       "      <td>0.049021</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.112734</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.214439</td>\n",
       "      <td>0.214235</td>\n",
       "      <td>0.096442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.555615</td>\n",
       "      <td>0.552881</td>\n",
       "      <td>0.628114</td>\n",
       "      <td>0.409525</td>\n",
       "      <td>0.753909</td>\n",
       "      <td>0.423920</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.931178</td>\n",
       "      <td>0.864473</td>\n",
       "      <td>0.344137</td>\n",
       "      <td>0.555615</td>\n",
       "      <td>0.183053</td>\n",
       "      <td>0.063270</td>\n",
       "      <td>0.082685</td>\n",
       "      <td>0.153368</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.128306</td>\n",
       "      <td>0.128398</td>\n",
       "      <td>0.214506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.468393</td>\n",
       "      <td>0.605621</td>\n",
       "      <td>0.465742</td>\n",
       "      <td>0.352403</td>\n",
       "      <td>0.693176</td>\n",
       "      <td>0.424407</td>\n",
       "      <td>0.042940</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>0.934280</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>0.043218</td>\n",
       "      <td>0.468393</td>\n",
       "      <td>0.267115</td>\n",
       "      <td>0.048190</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.080777</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.124017</td>\n",
       "      <td>0.124106</td>\n",
       "      <td>0.141951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.462690</td>\n",
       "      <td>0.645067</td>\n",
       "      <td>0.452699</td>\n",
       "      <td>0.335532</td>\n",
       "      <td>0.721318</td>\n",
       "      <td>0.469244</td>\n",
       "      <td>0.035768</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.924046</td>\n",
       "      <td>0.867521</td>\n",
       "      <td>0.387264</td>\n",
       "      <td>0.462690</td>\n",
       "      <td>0.203649</td>\n",
       "      <td>0.036097</td>\n",
       "      <td>0.626292</td>\n",
       "      <td>0.160637</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.229092</td>\n",
       "      <td>0.228898</td>\n",
       "      <td>0.094919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.669918</td>\n",
       "      <td>0.430077</td>\n",
       "      <td>0.719196</td>\n",
       "      <td>0.520440</td>\n",
       "      <td>0.809402</td>\n",
       "      <td>0.362420</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.816924</td>\n",
       "      <td>0.620554</td>\n",
       "      <td>0.785095</td>\n",
       "      <td>0.669918</td>\n",
       "      <td>0.417081</td>\n",
       "      <td>0.078355</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.430291</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.127949</td>\n",
       "      <td>0.128040</td>\n",
       "      <td>0.446763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.678842</td>\n",
       "      <td>0.501693</td>\n",
       "      <td>0.720312</td>\n",
       "      <td>0.521693</td>\n",
       "      <td>0.855473</td>\n",
       "      <td>0.405803</td>\n",
       "      <td>0.099075</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>0.828787</td>\n",
       "      <td>0.663124</td>\n",
       "      <td>0.178526</td>\n",
       "      <td>0.678842</td>\n",
       "      <td>0.259423</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.419655</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.308077</td>\n",
       "      <td>0.299356</td>\n",
       "      <td>0.149438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.637082</td>\n",
       "      <td>0.526755</td>\n",
       "      <td>0.718879</td>\n",
       "      <td>0.466875</td>\n",
       "      <td>0.803967</td>\n",
       "      <td>0.412844</td>\n",
       "      <td>0.125588</td>\n",
       "      <td>0.045655</td>\n",
       "      <td>0.872508</td>\n",
       "      <td>0.742281</td>\n",
       "      <td>0.178670</td>\n",
       "      <td>0.637082</td>\n",
       "      <td>0.255288</td>\n",
       "      <td>0.044015</td>\n",
       "      <td>0.812749</td>\n",
       "      <td>0.546969</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.319871</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.224492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.715351</td>\n",
       "      <td>0.489393</td>\n",
       "      <td>0.787119</td>\n",
       "      <td>0.534367</td>\n",
       "      <td>0.873710</td>\n",
       "      <td>0.410315</td>\n",
       "      <td>0.041076</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.821445</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>0.179033</td>\n",
       "      <td>0.715351</td>\n",
       "      <td>0.317225</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.483514</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.288778</td>\n",
       "      <td>0.288984</td>\n",
       "      <td>0.273259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.622796</td>\n",
       "      <td>0.583101</td>\n",
       "      <td>0.566762</td>\n",
       "      <td>0.494347</td>\n",
       "      <td>0.870504</td>\n",
       "      <td>0.448816</td>\n",
       "      <td>0.088638</td>\n",
       "      <td>0.018123</td>\n",
       "      <td>0.814901</td>\n",
       "      <td>0.681765</td>\n",
       "      <td>0.214137</td>\n",
       "      <td>0.622796</td>\n",
       "      <td>0.132657</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.033380</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.025733</td>\n",
       "      <td>0.025751</td>\n",
       "      <td>0.148390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.609097</td>\n",
       "      <td>0.575354</td>\n",
       "      <td>0.538038</td>\n",
       "      <td>0.467498</td>\n",
       "      <td>0.854048</td>\n",
       "      <td>0.460772</td>\n",
       "      <td>0.074098</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.804915</td>\n",
       "      <td>0.648669</td>\n",
       "      <td>0.214404</td>\n",
       "      <td>0.609097</td>\n",
       "      <td>0.153339</td>\n",
       "      <td>0.030580</td>\n",
       "      <td>0.731681</td>\n",
       "      <td>0.047036</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>0.142704</td>\n",
       "      <td>0.063855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.634056</td>\n",
       "      <td>0.567416</td>\n",
       "      <td>0.569806</td>\n",
       "      <td>0.499564</td>\n",
       "      <td>0.873178</td>\n",
       "      <td>0.445985</td>\n",
       "      <td>0.077008</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.790094</td>\n",
       "      <td>0.597217</td>\n",
       "      <td>0.214381</td>\n",
       "      <td>0.634056</td>\n",
       "      <td>0.189839</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.610344</td>\n",
       "      <td>0.062793</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.128663</td>\n",
       "      <td>0.128755</td>\n",
       "      <td>0.073065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.629914</td>\n",
       "      <td>0.604059</td>\n",
       "      <td>0.666381</td>\n",
       "      <td>0.484955</td>\n",
       "      <td>0.878095</td>\n",
       "      <td>0.465944</td>\n",
       "      <td>0.081716</td>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.767996</td>\n",
       "      <td>0.603187</td>\n",
       "      <td>0.214119</td>\n",
       "      <td>0.629914</td>\n",
       "      <td>0.208755</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>0.550543</td>\n",
       "      <td>0.077314</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.032523</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.252119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.668926</td>\n",
       "      <td>0.577918</td>\n",
       "      <td>0.632665</td>\n",
       "      <td>0.519769</td>\n",
       "      <td>0.916301</td>\n",
       "      <td>0.466803</td>\n",
       "      <td>0.070715</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>0.725853</td>\n",
       "      <td>0.543931</td>\n",
       "      <td>0.214132</td>\n",
       "      <td>0.668926</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.032784</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.068488</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.168692</td>\n",
       "      <td>0.168813</td>\n",
       "      <td>0.064287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.558102</td>\n",
       "      <td>0.537734</td>\n",
       "      <td>0.460107</td>\n",
       "      <td>0.800364</td>\n",
       "      <td>0.416386</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>0.627182</td>\n",
       "      <td>0.214076</td>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.038316</td>\n",
       "      <td>0.030422</td>\n",
       "      <td>0.536516</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.098350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.617913</td>\n",
       "      <td>0.585509</td>\n",
       "      <td>0.539774</td>\n",
       "      <td>0.500813</td>\n",
       "      <td>0.898735</td>\n",
       "      <td>0.469475</td>\n",
       "      <td>0.077350</td>\n",
       "      <td>0.008945</td>\n",
       "      <td>0.719887</td>\n",
       "      <td>0.559623</td>\n",
       "      <td>0.214086</td>\n",
       "      <td>0.617913</td>\n",
       "      <td>0.121563</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.509475</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.173526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.571680</td>\n",
       "      <td>0.601089</td>\n",
       "      <td>0.534862</td>\n",
       "      <td>0.488411</td>\n",
       "      <td>0.842854</td>\n",
       "      <td>0.428169</td>\n",
       "      <td>0.176715</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.798776</td>\n",
       "      <td>0.658199</td>\n",
       "      <td>0.214565</td>\n",
       "      <td>0.571680</td>\n",
       "      <td>0.238252</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.180486</td>\n",
       "      <td>0.180615</td>\n",
       "      <td>0.079249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.591879</td>\n",
       "      <td>0.588159</td>\n",
       "      <td>0.543632</td>\n",
       "      <td>0.479405</td>\n",
       "      <td>0.856871</td>\n",
       "      <td>0.451130</td>\n",
       "      <td>0.117595</td>\n",
       "      <td>0.031826</td>\n",
       "      <td>0.830169</td>\n",
       "      <td>0.703601</td>\n",
       "      <td>0.214160</td>\n",
       "      <td>0.591879</td>\n",
       "      <td>0.150476</td>\n",
       "      <td>0.030185</td>\n",
       "      <td>0.857144</td>\n",
       "      <td>0.045948</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>0.048283</td>\n",
       "      <td>0.135059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.614919</td>\n",
       "      <td>0.590974</td>\n",
       "      <td>0.701271</td>\n",
       "      <td>0.471344</td>\n",
       "      <td>0.848519</td>\n",
       "      <td>0.451409</td>\n",
       "      <td>0.119368</td>\n",
       "      <td>0.033518</td>\n",
       "      <td>0.784264</td>\n",
       "      <td>0.628778</td>\n",
       "      <td>0.214164</td>\n",
       "      <td>0.614919</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>0.033121</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.047672</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.164761</td>\n",
       "      <td>0.164878</td>\n",
       "      <td>0.054528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.612285</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.528248</td>\n",
       "      <td>0.508145</td>\n",
       "      <td>0.890882</td>\n",
       "      <td>0.454235</td>\n",
       "      <td>0.084940</td>\n",
       "      <td>0.009385</td>\n",
       "      <td>0.672395</td>\n",
       "      <td>0.546944</td>\n",
       "      <td>0.457670</td>\n",
       "      <td>0.612285</td>\n",
       "      <td>0.412108</td>\n",
       "      <td>0.031221</td>\n",
       "      <td>0.694572</td>\n",
       "      <td>0.111022</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.032166</td>\n",
       "      <td>0.032189</td>\n",
       "      <td>0.426175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.604348</td>\n",
       "      <td>0.562150</td>\n",
       "      <td>0.522536</td>\n",
       "      <td>0.493469</td>\n",
       "      <td>0.854811</td>\n",
       "      <td>0.434508</td>\n",
       "      <td>0.059288</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.719605</td>\n",
       "      <td>0.623530</td>\n",
       "      <td>0.481366</td>\n",
       "      <td>0.604348</td>\n",
       "      <td>0.354185</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.098603</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.030736</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.412686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.666624</td>\n",
       "      <td>0.541778</td>\n",
       "      <td>0.525118</td>\n",
       "      <td>0.523282</td>\n",
       "      <td>0.908936</td>\n",
       "      <td>0.456007</td>\n",
       "      <td>0.076864</td>\n",
       "      <td>0.007741</td>\n",
       "      <td>0.470377</td>\n",
       "      <td>0.343089</td>\n",
       "      <td>0.477064</td>\n",
       "      <td>0.666624</td>\n",
       "      <td>0.390189</td>\n",
       "      <td>0.037385</td>\n",
       "      <td>0.424296</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>0.022890</td>\n",
       "      <td>0.353121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.656377</td>\n",
       "      <td>0.316581</td>\n",
       "      <td>0.165369</td>\n",
       "      <td>0.680861</td>\n",
       "      <td>0.606933</td>\n",
       "      <td>0.027815</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.885486</td>\n",
       "      <td>0.809456</td>\n",
       "      <td>0.034826</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.735035</td>\n",
       "      <td>0.033802</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.189509</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.195139</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.196550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.346646</td>\n",
       "      <td>0.578478</td>\n",
       "      <td>0.332763</td>\n",
       "      <td>0.198101</td>\n",
       "      <td>0.608568</td>\n",
       "      <td>0.502780</td>\n",
       "      <td>0.023254</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.933107</td>\n",
       "      <td>0.875355</td>\n",
       "      <td>0.710784</td>\n",
       "      <td>0.346646</td>\n",
       "      <td>0.627614</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.294858</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.225518</td>\n",
       "      <td>0.225680</td>\n",
       "      <td>0.184162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>0.411198</td>\n",
       "      <td>0.629952</td>\n",
       "      <td>0.464936</td>\n",
       "      <td>0.188818</td>\n",
       "      <td>0.676922</td>\n",
       "      <td>0.578732</td>\n",
       "      <td>0.037892</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.958698</td>\n",
       "      <td>0.934545</td>\n",
       "      <td>0.765827</td>\n",
       "      <td>0.411198</td>\n",
       "      <td>0.681313</td>\n",
       "      <td>0.101899</td>\n",
       "      <td>0.929285</td>\n",
       "      <td>0.205534</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.224446</td>\n",
       "      <td>0.224607</td>\n",
       "      <td>0.096576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>0.368275</td>\n",
       "      <td>0.739570</td>\n",
       "      <td>0.393621</td>\n",
       "      <td>0.096263</td>\n",
       "      <td>0.698409</td>\n",
       "      <td>0.695807</td>\n",
       "      <td>0.071389</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.883405</td>\n",
       "      <td>0.824452</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.368275</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.034670</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.061328</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.297905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>0.308980</td>\n",
       "      <td>0.689685</td>\n",
       "      <td>0.267377</td>\n",
       "      <td>0.113951</td>\n",
       "      <td>0.620997</td>\n",
       "      <td>0.602331</td>\n",
       "      <td>0.065852</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>0.897185</td>\n",
       "      <td>0.845577</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>0.308980</td>\n",
       "      <td>0.703023</td>\n",
       "      <td>0.034321</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.098357</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>0.397806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>0.416838</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.591078</td>\n",
       "      <td>0.138808</td>\n",
       "      <td>0.687495</td>\n",
       "      <td>0.640985</td>\n",
       "      <td>0.041911</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.867516</td>\n",
       "      <td>0.787002</td>\n",
       "      <td>0.650581</td>\n",
       "      <td>0.416838</td>\n",
       "      <td>0.674452</td>\n",
       "      <td>0.249125</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.212281</td>\n",
       "      <td>0.023656</td>\n",
       "      <td>0.229807</td>\n",
       "      <td>0.229614</td>\n",
       "      <td>0.130040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>0.245025</td>\n",
       "      <td>0.707804</td>\n",
       "      <td>0.148718</td>\n",
       "      <td>0.060553</td>\n",
       "      <td>0.590383</td>\n",
       "      <td>0.628159</td>\n",
       "      <td>0.085244</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.789265</td>\n",
       "      <td>0.726918</td>\n",
       "      <td>0.042590</td>\n",
       "      <td>0.245025</td>\n",
       "      <td>0.633422</td>\n",
       "      <td>0.032952</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.257470</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.241601</td>\n",
       "      <td>0.241774</td>\n",
       "      <td>0.201542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>0.203251</td>\n",
       "      <td>0.689021</td>\n",
       "      <td>0.096460</td>\n",
       "      <td>0.067544</td>\n",
       "      <td>0.476656</td>\n",
       "      <td>0.510582</td>\n",
       "      <td>0.070241</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>0.752007</td>\n",
       "      <td>0.668949</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>0.203251</td>\n",
       "      <td>0.702044</td>\n",
       "      <td>0.124892</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.108869</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.477333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>0.402962</td>\n",
       "      <td>0.646228</td>\n",
       "      <td>0.483148</td>\n",
       "      <td>0.169168</td>\n",
       "      <td>0.669440</td>\n",
       "      <td>0.591905</td>\n",
       "      <td>0.052887</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.903572</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>0.706116</td>\n",
       "      <td>0.402962</td>\n",
       "      <td>0.697242</td>\n",
       "      <td>0.304624</td>\n",
       "      <td>0.771005</td>\n",
       "      <td>0.096857</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.038599</td>\n",
       "      <td>0.038627</td>\n",
       "      <td>0.424819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>0.435410</td>\n",
       "      <td>0.680948</td>\n",
       "      <td>0.483173</td>\n",
       "      <td>0.213921</td>\n",
       "      <td>0.664581</td>\n",
       "      <td>0.540660</td>\n",
       "      <td>0.060769</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.943815</td>\n",
       "      <td>0.927017</td>\n",
       "      <td>0.698269</td>\n",
       "      <td>0.435410</td>\n",
       "      <td>0.744753</td>\n",
       "      <td>0.099681</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.070143</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.036097</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.377150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>0.328320</td>\n",
       "      <td>0.760069</td>\n",
       "      <td>0.236116</td>\n",
       "      <td>0.090205</td>\n",
       "      <td>0.686410</td>\n",
       "      <td>0.690468</td>\n",
       "      <td>0.060540</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.858738</td>\n",
       "      <td>0.797607</td>\n",
       "      <td>0.040214</td>\n",
       "      <td>0.328320</td>\n",
       "      <td>0.576192</td>\n",
       "      <td>0.050112</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.166078</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.134382</td>\n",
       "      <td>0.134478</td>\n",
       "      <td>0.253374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>0.241223</td>\n",
       "      <td>0.626167</td>\n",
       "      <td>0.193342</td>\n",
       "      <td>0.083619</td>\n",
       "      <td>0.541292</td>\n",
       "      <td>0.556560</td>\n",
       "      <td>0.059144</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.837299</td>\n",
       "      <td>0.734581</td>\n",
       "      <td>0.042538</td>\n",
       "      <td>0.241223</td>\n",
       "      <td>0.696794</td>\n",
       "      <td>0.086704</td>\n",
       "      <td>0.880415</td>\n",
       "      <td>0.246669</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.252680</td>\n",
       "      <td>0.252861</td>\n",
       "      <td>0.182855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>0.463467</td>\n",
       "      <td>0.754903</td>\n",
       "      <td>0.600695</td>\n",
       "      <td>0.175292</td>\n",
       "      <td>0.773516</td>\n",
       "      <td>0.686486</td>\n",
       "      <td>0.028276</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.941274</td>\n",
       "      <td>0.871067</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.463467</td>\n",
       "      <td>0.738875</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.117481</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.142244</td>\n",
       "      <td>0.142346</td>\n",
       "      <td>0.103037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>0.348433</td>\n",
       "      <td>0.742665</td>\n",
       "      <td>0.293149</td>\n",
       "      <td>0.106806</td>\n",
       "      <td>0.676216</td>\n",
       "      <td>0.663321</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.899059</td>\n",
       "      <td>0.844610</td>\n",
       "      <td>0.065824</td>\n",
       "      <td>0.348433</td>\n",
       "      <td>0.724324</td>\n",
       "      <td>0.070609</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.208374</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.223731</td>\n",
       "      <td>0.223891</td>\n",
       "      <td>0.138053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>0.521191</td>\n",
       "      <td>0.665450</td>\n",
       "      <td>0.679152</td>\n",
       "      <td>0.242730</td>\n",
       "      <td>0.767133</td>\n",
       "      <td>0.610174</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.867637</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521191</td>\n",
       "      <td>0.705268</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>0.857144</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.275554</td>\n",
       "      <td>0.266810</td>\n",
       "      <td>0.392225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>0.709531</td>\n",
       "      <td>0.181296</td>\n",
       "      <td>0.731832</td>\n",
       "      <td>0.680666</td>\n",
       "      <td>0.704236</td>\n",
       "      <td>0.093818</td>\n",
       "      <td>0.074679</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.544758</td>\n",
       "      <td>0.247881</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.709531</td>\n",
       "      <td>0.590392</td>\n",
       "      <td>0.102464</td>\n",
       "      <td>0.880415</td>\n",
       "      <td>0.461844</td>\n",
       "      <td>0.350538</td>\n",
       "      <td>0.319871</td>\n",
       "      <td>0.312947</td>\n",
       "      <td>0.253062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>0.759035</td>\n",
       "      <td>0.280362</td>\n",
       "      <td>0.770517</td>\n",
       "      <td>0.713635</td>\n",
       "      <td>0.820683</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>0.029449</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.743619</td>\n",
       "      <td>0.397325</td>\n",
       "      <td>0.631790</td>\n",
       "      <td>0.759035</td>\n",
       "      <td>0.620447</td>\n",
       "      <td>0.275163</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.241006</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.319871</td>\n",
       "      <td>0.313662</td>\n",
       "      <td>0.098896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>0.657390</td>\n",
       "      <td>0.292183</td>\n",
       "      <td>0.613042</td>\n",
       "      <td>0.603273</td>\n",
       "      <td>0.701252</td>\n",
       "      <td>0.171394</td>\n",
       "      <td>0.084575</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.352695</td>\n",
       "      <td>0.546543</td>\n",
       "      <td>0.657390</td>\n",
       "      <td>0.548219</td>\n",
       "      <td>0.080813</td>\n",
       "      <td>0.857144</td>\n",
       "      <td>0.213605</td>\n",
       "      <td>0.316129</td>\n",
       "      <td>0.280915</td>\n",
       "      <td>0.274678</td>\n",
       "      <td>0.108638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>0.767105</td>\n",
       "      <td>0.182580</td>\n",
       "      <td>0.761829</td>\n",
       "      <td>0.720045</td>\n",
       "      <td>0.802202</td>\n",
       "      <td>0.147895</td>\n",
       "      <td>0.041743</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.600301</td>\n",
       "      <td>0.236988</td>\n",
       "      <td>0.628989</td>\n",
       "      <td>0.767105</td>\n",
       "      <td>0.747721</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.198632</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.270550</td>\n",
       "      <td>0.270744</td>\n",
       "      <td>0.133405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>0.775704</td>\n",
       "      <td>0.239040</td>\n",
       "      <td>0.771747</td>\n",
       "      <td>0.709913</td>\n",
       "      <td>0.851002</td>\n",
       "      <td>0.205763</td>\n",
       "      <td>0.038204</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.677256</td>\n",
       "      <td>0.260622</td>\n",
       "      <td>0.745789</td>\n",
       "      <td>0.775704</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.055533</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.294164</td>\n",
       "      <td>0.367742</td>\n",
       "      <td>0.315225</td>\n",
       "      <td>0.307940</td>\n",
       "      <td>0.156089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>0.681449</td>\n",
       "      <td>0.229530</td>\n",
       "      <td>0.685551</td>\n",
       "      <td>0.632293</td>\n",
       "      <td>0.714460</td>\n",
       "      <td>0.154031</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.655395</td>\n",
       "      <td>0.343707</td>\n",
       "      <td>0.632287</td>\n",
       "      <td>0.681449</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>0.045664</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.183906</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.156183</td>\n",
       "      <td>0.156295</td>\n",
       "      <td>0.178580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>0.611214</td>\n",
       "      <td>0.696308</td>\n",
       "      <td>0.711311</td>\n",
       "      <td>0.385762</td>\n",
       "      <td>0.855804</td>\n",
       "      <td>0.547460</td>\n",
       "      <td>0.038183</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.893997</td>\n",
       "      <td>0.831185</td>\n",
       "      <td>0.658723</td>\n",
       "      <td>0.611214</td>\n",
       "      <td>0.699167</td>\n",
       "      <td>0.056217</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.332377</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.268763</td>\n",
       "      <td>0.268956</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>0.530825</td>\n",
       "      <td>0.730405</td>\n",
       "      <td>0.699283</td>\n",
       "      <td>0.234424</td>\n",
       "      <td>0.812295</td>\n",
       "      <td>0.662615</td>\n",
       "      <td>0.031401</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.918012</td>\n",
       "      <td>0.877992</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.530825</td>\n",
       "      <td>0.802074</td>\n",
       "      <td>0.070254</td>\n",
       "      <td>0.929285</td>\n",
       "      <td>0.257275</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.182988</td>\n",
       "      <td>0.183119</td>\n",
       "      <td>0.206162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3161</th>\n",
       "      <td>0.620005</td>\n",
       "      <td>0.648786</td>\n",
       "      <td>0.692518</td>\n",
       "      <td>0.456394</td>\n",
       "      <td>0.850891</td>\n",
       "      <td>0.469255</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.871596</td>\n",
       "      <td>0.771260</td>\n",
       "      <td>0.576807</td>\n",
       "      <td>0.620005</td>\n",
       "      <td>0.784909</td>\n",
       "      <td>0.773133</td>\n",
       "      <td>0.857144</td>\n",
       "      <td>0.137718</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.033238</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.361354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>0.503683</td>\n",
       "      <td>0.765260</td>\n",
       "      <td>0.689148</td>\n",
       "      <td>0.168010</td>\n",
       "      <td>0.786867</td>\n",
       "      <td>0.707007</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.864402</td>\n",
       "      <td>0.772832</td>\n",
       "      <td>0.769578</td>\n",
       "      <td>0.503683</td>\n",
       "      <td>0.769339</td>\n",
       "      <td>0.153011</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.178327</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.136526</td>\n",
       "      <td>0.136624</td>\n",
       "      <td>0.277704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>0.436911</td>\n",
       "      <td>0.684871</td>\n",
       "      <td>0.570361</td>\n",
       "      <td>0.198513</td>\n",
       "      <td>0.686256</td>\n",
       "      <td>0.577704</td>\n",
       "      <td>0.046854</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.921665</td>\n",
       "      <td>0.901057</td>\n",
       "      <td>0.717272</td>\n",
       "      <td>0.436911</td>\n",
       "      <td>0.698762</td>\n",
       "      <td>0.380813</td>\n",
       "      <td>0.904450</td>\n",
       "      <td>0.279703</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.192280</td>\n",
       "      <td>0.192418</td>\n",
       "      <td>0.173674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>0.362946</td>\n",
       "      <td>0.731172</td>\n",
       "      <td>0.262871</td>\n",
       "      <td>0.171937</td>\n",
       "      <td>0.702595</td>\n",
       "      <td>0.621185</td>\n",
       "      <td>0.015961</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.912549</td>\n",
       "      <td>0.834545</td>\n",
       "      <td>0.048868</td>\n",
       "      <td>0.362946</td>\n",
       "      <td>0.732760</td>\n",
       "      <td>0.126776</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.305791</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.167977</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.298053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>0.484949</td>\n",
       "      <td>0.799042</td>\n",
       "      <td>0.690337</td>\n",
       "      <td>0.134329</td>\n",
       "      <td>0.786967</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.050161</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.855587</td>\n",
       "      <td>0.765849</td>\n",
       "      <td>0.028592</td>\n",
       "      <td>0.484949</td>\n",
       "      <td>0.847759</td>\n",
       "      <td>0.153011</td>\n",
       "      <td>0.981526</td>\n",
       "      <td>0.164908</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.134024</td>\n",
       "      <td>0.134120</td>\n",
       "      <td>0.208885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>0.492516</td>\n",
       "      <td>0.745692</td>\n",
       "      <td>0.695311</td>\n",
       "      <td>0.175136</td>\n",
       "      <td>0.767804</td>\n",
       "      <td>0.681107</td>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.870307</td>\n",
       "      <td>0.792241</td>\n",
       "      <td>0.757865</td>\n",
       "      <td>0.492516</td>\n",
       "      <td>0.641561</td>\n",
       "      <td>0.127158</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.265621</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.164046</td>\n",
       "      <td>0.164163</td>\n",
       "      <td>0.333559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.768964</td>\n",
       "      <td>0.687590</td>\n",
       "      <td>0.282629</td>\n",
       "      <td>0.901780</td>\n",
       "      <td>0.699289</td>\n",
       "      <td>0.045203</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.822610</td>\n",
       "      <td>0.700510</td>\n",
       "      <td>0.956078</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.714235</td>\n",
       "      <td>0.270097</td>\n",
       "      <td>0.954963</td>\n",
       "      <td>0.074312</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.375386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0     0.096419  0.473409  0.084125  0.060063  0.204956  0.254828  0.367853   \n",
       "1     0.125828  0.505075  0.116900  0.077635  0.215683  0.246961  0.644279   \n",
       "2     0.179222  0.675536  0.102873  0.034284  0.385912  0.457148  0.885255   \n",
       "3     0.528261  0.554611  0.587559  0.389906  0.715802  0.407358  0.031549   \n",
       "4     0.452195  0.627209  0.454272  0.317627  0.707515  0.474474  0.027742   \n",
       "5     0.441173  0.631448  0.432029  0.274076  0.722901  0.534679  0.051782   \n",
       "6     0.526061  0.578887  0.595932  0.375003  0.706098  0.413441  0.040161   \n",
       "7     0.572113  0.602659  0.532916  0.446359  0.819942  0.449670  0.036301   \n",
       "8     0.485814  0.615573  0.509942  0.356014  0.718545  0.445258  0.027701   \n",
       "9     0.448457  0.639632  0.441466  0.304920  0.689783  0.470487  0.030322   \n",
       "10    0.555615  0.552881  0.628114  0.409525  0.753909  0.423920  0.024223   \n",
       "11    0.468393  0.605621  0.465742  0.352403  0.693176  0.424407  0.042940   \n",
       "12    0.462690  0.645067  0.452699  0.335532  0.721318  0.469244  0.035768   \n",
       "13    0.669918  0.430077  0.719196  0.520440  0.809402  0.362420  0.035499   \n",
       "14    0.678842  0.501693  0.720312  0.521693  0.855473  0.405803  0.099075   \n",
       "15    0.637082  0.526755  0.718879  0.466875  0.803967  0.412844  0.125588   \n",
       "16    0.715351  0.489393  0.787119  0.534367  0.873710  0.410315  0.041076   \n",
       "17    0.622796  0.583101  0.566762  0.494347  0.870504  0.448816  0.088638   \n",
       "18    0.609097  0.575354  0.538038  0.467498  0.854048  0.460772  0.074098   \n",
       "19    0.634056  0.567416  0.569806  0.499564  0.873178  0.445985  0.077008   \n",
       "20    0.629914  0.604059  0.666381  0.484955  0.878095  0.465944  0.081716   \n",
       "21    0.668926  0.577918  0.632665  0.519769  0.916301  0.466803  0.070715   \n",
       "22    0.586382  0.558102  0.537734  0.460107  0.800364  0.416386  0.099640   \n",
       "23    0.617913  0.585509  0.539774  0.500813  0.898735  0.469475  0.077350   \n",
       "24    0.571680  0.601089  0.534862  0.488411  0.842854  0.428169  0.176715   \n",
       "25    0.591879  0.588159  0.543632  0.479405  0.856871  0.451130  0.117595   \n",
       "26    0.614919  0.590974  0.701271  0.471344  0.848519  0.451409  0.119368   \n",
       "27    0.612285  0.551181  0.528248  0.508145  0.890882  0.454235  0.084940   \n",
       "28    0.604348  0.562150  0.522536  0.493469  0.854811  0.434508  0.059288   \n",
       "29    0.666624  0.541778  0.525118  0.523282  0.908936  0.456007  0.076864   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3138  0.354709  0.656377  0.316581  0.165369  0.680861  0.606933  0.027815   \n",
       "3139  0.346646  0.578478  0.332763  0.198101  0.608568  0.502780  0.023254   \n",
       "3140  0.411198  0.629952  0.464936  0.188818  0.676922  0.578732  0.037892   \n",
       "3141  0.368275  0.739570  0.393621  0.096263  0.698409  0.695807  0.071389   \n",
       "3142  0.308980  0.689685  0.267377  0.113951  0.620997  0.602331  0.065852   \n",
       "3143  0.416838  0.686905  0.591078  0.138808  0.687495  0.640985  0.041911   \n",
       "3144  0.245025  0.707804  0.148718  0.060553  0.590383  0.628159  0.085244   \n",
       "3145  0.203251  0.689021  0.096460  0.067544  0.476656  0.510582  0.070241   \n",
       "3146  0.402962  0.646228  0.483148  0.169168  0.669440  0.591905  0.052887   \n",
       "3147  0.435410  0.680948  0.483173  0.213921  0.664581  0.540660  0.060769   \n",
       "3148  0.328320  0.760069  0.236116  0.090205  0.686410  0.690468  0.060540   \n",
       "3149  0.241223  0.626167  0.193342  0.083619  0.541292  0.556560  0.059144   \n",
       "3150  0.463467  0.754903  0.600695  0.175292  0.773516  0.686486  0.028276   \n",
       "3151  0.348433  0.742665  0.293149  0.106806  0.676216  0.663321  0.061200   \n",
       "3152  0.521191  0.665450  0.679152  0.242730  0.767133  0.610174  0.031758   \n",
       "3153  0.709531  0.181296  0.731832  0.680666  0.704236  0.093818  0.074679   \n",
       "3154  0.759035  0.280362  0.770517  0.713635  0.820683  0.172485  0.029449   \n",
       "3155  0.657390  0.292183  0.613042  0.603273  0.701252  0.171394  0.084575   \n",
       "3156  0.767105  0.182580  0.761829  0.720045  0.802202  0.147895  0.041743   \n",
       "3157  0.775704  0.239040  0.771747  0.709913  0.851002  0.205763  0.038204   \n",
       "3158  0.681449  0.229530  0.685551  0.632293  0.714460  0.154031  0.055298   \n",
       "3159  0.611214  0.696308  0.711311  0.385762  0.855804  0.547460  0.038183   \n",
       "3160  0.530825  0.730405  0.699283  0.234424  0.812295  0.662615  0.031401   \n",
       "3161  0.620005  0.648786  0.692518  0.456394  0.850891  0.469255  0.035754   \n",
       "3162  0.503683  0.765260  0.689148  0.168010  0.786867  0.707007  0.035949   \n",
       "3163  0.436911  0.684871  0.570361  0.198513  0.686256  0.577704  0.046854   \n",
       "3164  0.362946  0.731172  0.262871  0.171937  0.702595  0.621185  0.015961   \n",
       "3165  0.484949  0.799042  0.690337  0.134329  0.786967  0.742124  0.050161   \n",
       "3166  0.492516  0.745692  0.695311  0.175136  0.767804  0.681107  0.041908   \n",
       "3167  0.595700  0.768964  0.687590  0.282629  0.901780  0.699289  0.045203   \n",
       "\n",
       "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
       "0     0.208279  0.635798  0.564526  0.000000  0.096419  0.157706  0.030501   \n",
       "1     0.483766  0.630964  0.591578  0.000000  0.125828  0.287642  0.031140   \n",
       "2     0.782275  0.442738  0.548382  0.000000  0.179222  0.236945  0.030264   \n",
       "3     0.001613  0.923261  0.856457  0.299565  0.528261  0.183442  0.041287   \n",
       "4     0.001732  0.958736  0.926348  0.372362  0.452195  0.279190  0.036829   \n",
       "5     0.004773  0.922681  0.870197  0.401984  0.441173  0.299699  0.037761   \n",
       "6     0.002997  0.940728  0.900382  0.307846  0.526061  0.276701  0.084682   \n",
       "7     0.002064  0.906544  0.847309  0.458300  0.572113  0.205893  0.041084   \n",
       "8     0.001531  0.953672  0.910746  0.782511  0.485814  0.226085  0.042110   \n",
       "9     0.002079  0.972260  0.952323  0.041781  0.448457  0.276351  0.049021   \n",
       "10    0.001458  0.931178  0.864473  0.344137  0.555615  0.183053  0.063270   \n",
       "11    0.003230  0.934280  0.887237  0.043218  0.468393  0.267115  0.048190   \n",
       "12    0.002249  0.924046  0.867521  0.387264  0.462690  0.203649  0.036097   \n",
       "13    0.002606  0.816924  0.620554  0.785095  0.669918  0.417081  0.078355   \n",
       "14    0.025480  0.828787  0.663124  0.178526  0.678842  0.259423  0.056911   \n",
       "15    0.045655  0.872508  0.742281  0.178670  0.637082  0.255288  0.044015   \n",
       "16    0.004410  0.821445  0.622700  0.179033  0.715351  0.317225  0.039982   \n",
       "17    0.018123  0.814901  0.681765  0.214137  0.622796  0.132657  0.030342   \n",
       "18    0.012555  0.804915  0.648669  0.214404  0.609097  0.153339  0.030580   \n",
       "19    0.014370  0.790094  0.597217  0.214381  0.634056  0.189839  0.030501   \n",
       "20    0.013774  0.767996  0.603187  0.214119  0.629914  0.208755  0.030819   \n",
       "21    0.007811  0.725853  0.543931  0.214132  0.668926  0.236600  0.032784   \n",
       "22    0.020332  0.774059  0.627182  0.214076  0.586382  0.038316  0.030422   \n",
       "23    0.008945  0.719887  0.559623  0.214086  0.617913  0.121563  0.030501   \n",
       "24    0.063802  0.798776  0.658199  0.214565  0.571680  0.238252  0.032533   \n",
       "25    0.031826  0.830169  0.703601  0.214160  0.591879  0.150476  0.030185   \n",
       "26    0.033518  0.784264  0.628778  0.214164  0.614919  0.147667  0.033121   \n",
       "27    0.009385  0.672395  0.546944  0.457670  0.612285  0.412108  0.031221   \n",
       "28    0.004653  0.719605  0.623530  0.481366  0.604348  0.354185  0.033290   \n",
       "29    0.007741  0.470377  0.343089  0.477064  0.666624  0.390189  0.037385   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3138  0.001293  0.885486  0.809456  0.034826  0.354709  0.735035  0.033802   \n",
       "3139  0.000935  0.933107  0.875355  0.710784  0.346646  0.627614  0.041903   \n",
       "3140  0.002687  0.958698  0.934545  0.765827  0.411198  0.681313  0.101899   \n",
       "3141  0.007934  0.883405  0.824452  0.016440  0.368275  0.672676  0.034670   \n",
       "3142  0.007046  0.897185  0.845577  0.030329  0.308980  0.703023  0.034321   \n",
       "3143  0.002508  0.867516  0.787002  0.650581  0.416838  0.674452  0.249125   \n",
       "3144  0.008251  0.789265  0.726918  0.042590  0.245025  0.633422  0.032952   \n",
       "3145  0.005438  0.752007  0.668949  0.051982  0.203251  0.702044  0.124892   \n",
       "3146  0.004537  0.903572  0.849573  0.706116  0.402962  0.697242  0.304624   \n",
       "3147  0.007247  0.943815  0.927017  0.698269  0.435410  0.744753  0.099681   \n",
       "3148  0.004941  0.858738  0.797607  0.040214  0.328320  0.576192  0.050112   \n",
       "3149  0.004706  0.837299  0.734581  0.042538  0.241223  0.696794  0.086704   \n",
       "3150  0.001619  0.941274  0.871067  0.031008  0.463467  0.738875  0.048072   \n",
       "3151  0.005744  0.899059  0.844610  0.065824  0.348433  0.724324  0.070609   \n",
       "3152  0.001492  0.867637  0.764286  0.000000  0.521191  0.705268  0.217043   \n",
       "3153  0.006821  0.544758  0.247881  0.698630  0.709531  0.590392  0.102464   \n",
       "3154  0.001274  0.743619  0.397325  0.631790  0.759035  0.620447  0.275163   \n",
       "3155  0.010413  0.627900  0.352695  0.546543  0.657390  0.548219  0.080813   \n",
       "3156  0.002200  0.600301  0.236988  0.628989  0.767105  0.747721  0.118085   \n",
       "3157  0.002082  0.677256  0.260622  0.745789  0.775704  0.501000  0.055533   \n",
       "3158  0.004141  0.655395  0.343707  0.632287  0.681449  0.514479  0.045664   \n",
       "3159  0.002302  0.893997  0.831185  0.658723  0.611214  0.699167  0.056217   \n",
       "3160  0.001710  0.918012  0.877992  0.823383  0.530825  0.802074  0.070254   \n",
       "3161  0.002572  0.871596  0.771260  0.576807  0.620005  0.784909  0.773133   \n",
       "3162  0.002333  0.864402  0.772832  0.769578  0.503683  0.769339  0.153011   \n",
       "3163  0.003489  0.921665  0.901057  0.717272  0.436911  0.698762  0.380813   \n",
       "3164  0.000333  0.912549  0.834545  0.048868  0.362946  0.732760  0.126776   \n",
       "3165  0.003469  0.855587  0.765849  0.028592  0.484949  0.847759  0.153011   \n",
       "3166  0.002539  0.870307  0.792241  0.757865  0.492516  0.641561  0.127158   \n",
       "3167  0.002830  0.822610  0.700510  0.956078  0.595700  0.714235  0.270097   \n",
       "\n",
       "        maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
       "0     0.981526  0.000000  0.006452  0.000000  0.000000  0.000000  \n",
       "1     0.834600  0.000407  0.006452  0.002144  0.002146  0.056449  \n",
       "2     0.954963  0.000060  0.006452  0.000357  0.000358  0.049885  \n",
       "3     0.834600  0.065659  0.006452  0.025375  0.025393  0.265043  \n",
       "4     0.929285  0.238994  0.006452  0.250536  0.250715  0.223380  \n",
       "5     0.857144  0.098448  0.006452  0.124375  0.124464  0.134238  \n",
       "6     0.929285  0.159942  0.006452  0.242673  0.242847  0.132985  \n",
       "7     0.233218  0.099505  0.006452  0.024303  0.024320  0.304531  \n",
       "8     0.834600  0.111416  0.006452  0.098642  0.098712  0.159026  \n",
       "9     0.904450  0.112734  0.023656  0.214439  0.214235  0.096442  \n",
       "10    0.082685  0.153368  0.006452  0.128306  0.128398  0.214506  \n",
       "11    0.904450  0.080777  0.006452  0.124017  0.124106  0.141951  \n",
       "12    0.626292  0.160637  0.023656  0.229092  0.228898  0.094919  \n",
       "13    0.981526  0.430291  0.006452  0.127949  0.128040  0.446763  \n",
       "14    0.981526  0.419655  0.436559  0.308077  0.299356  0.149438  \n",
       "15    0.812749  0.546969  0.006452  0.319871  0.320100  0.224492  \n",
       "16    0.981526  0.483514  0.006452  0.288778  0.288984  0.273259  \n",
       "17    0.904450  0.033380  0.006452  0.025733  0.025751  0.148390  \n",
       "18    0.731681  0.047036  0.006452  0.142602  0.142704  0.063855  \n",
       "19    0.610344  0.062793  0.006452  0.128663  0.128755  0.073065  \n",
       "20    0.550543  0.077314  0.006452  0.032523  0.032546  0.252119  \n",
       "21    0.981526  0.068488  0.006452  0.168692  0.168813  0.064287  \n",
       "22    0.536516  0.017563  0.006452  0.020014  0.020029  0.098350  \n",
       "23    0.509475  0.031781  0.006452  0.025375  0.025393  0.173526  \n",
       "24    0.981526  0.067442  0.006452  0.180486  0.180615  0.079249  \n",
       "25    0.857144  0.045948  0.006452  0.048249  0.048283  0.135059  \n",
       "26    0.954963  0.047672  0.006452  0.164761  0.164878  0.054528  \n",
       "27    0.694572  0.111022  0.006452  0.032166  0.032189  0.426175  \n",
       "28    0.904450  0.098603  0.006452  0.030736  0.030758  0.412686  \n",
       "29    0.424296  0.076970  0.006452  0.022873  0.022890  0.353121  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "3138  0.954963  0.189509  0.006452  0.195139  0.195279  0.196550  \n",
       "3139  0.904450  0.294858  0.006452  0.225518  0.225680  0.184162  \n",
       "3140  0.929285  0.205534  0.006452  0.224446  0.224607  0.096576  \n",
       "3141  0.954963  0.061328  0.006452  0.033953  0.033977  0.297905  \n",
       "3142  0.981526  0.098357  0.006452  0.038956  0.038984  0.397806  \n",
       "3143  0.954963  0.212281  0.023656  0.229807  0.229614  0.130040  \n",
       "3144  0.981526  0.257470  0.006452  0.241601  0.241774  0.201542  \n",
       "3145  0.981526  0.108869  0.006452  0.033953  0.033977  0.477333  \n",
       "3146  0.771005  0.096857  0.006452  0.038599  0.038627  0.424819  \n",
       "3147  0.981526  0.070143  0.006452  0.036097  0.036123  0.377150  \n",
       "3148  0.981526  0.166078  0.006452  0.134382  0.134478  0.253374  \n",
       "3149  0.880415  0.246669  0.006452  0.252680  0.252861  0.182855  \n",
       "3150  0.981526  0.117481  0.006452  0.142244  0.142346  0.103037  \n",
       "3151  0.904450  0.208374  0.006452  0.223731  0.223891  0.138053  \n",
       "3152  0.857144  0.458800  0.436559  0.275554  0.266810  0.392225  \n",
       "3153  0.880415  0.461844  0.350538  0.319871  0.312947  0.253062  \n",
       "3154  0.904450  0.241006  0.316129  0.319871  0.313662  0.098896  \n",
       "3155  0.857144  0.213605  0.316129  0.280915  0.274678  0.108638  \n",
       "3156  0.981526  0.198632  0.006452  0.270550  0.270744  0.133405  \n",
       "3157  0.904450  0.294164  0.367742  0.315225  0.307940  0.156089  \n",
       "3158  0.904450  0.183906  0.006452  0.156183  0.156295  0.178580  \n",
       "3159  0.954963  0.332377  0.006452  0.268763  0.268956  0.288100  \n",
       "3160  0.929285  0.257275  0.006452  0.182988  0.183119  0.206162  \n",
       "3161  0.857144  0.137718  0.006452  0.033238  0.033262  0.361354  \n",
       "3162  0.981526  0.178327  0.006452  0.136526  0.136624  0.277704  \n",
       "3163  0.904450  0.279703  0.006452  0.192280  0.192418  0.173674  \n",
       "3164  0.981526  0.305791  0.075269  0.167977  0.166667  0.298053  \n",
       "3165  0.981526  0.164908  0.006452  0.134024  0.134120  0.208885  \n",
       "3166  0.834600  0.265621  0.006452  0.164046  0.164163  0.333559  \n",
       "3167  0.954963  0.074312  0.006452  0.025018  0.025036  0.375386  \n",
       "\n",
       "[3168 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "for col in x.columns:\n",
    "    all_features.append(tf.feature_column.numeric_column(col))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='meanfreq', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='sd', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='median', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='Q25', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='Q75', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='IQR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='skew', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='kurt', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='sp.ent', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='sfm', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='mode', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='centroid', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='meanfun', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='minfun', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='maxfun', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='meandom', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='mindom', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='maxdom', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dfrange', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='modindx', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_classe = all_classe.append(tf.feature_column.numeric_column(x))\n",
    "#all_classe = [tf.feature_column.numeric_column(\"y\", shape=[1])]\n",
    "\n",
    "all_classes = tf.feature_column.numeric_column(key=\"label\",shape = 10)\n",
    "\n",
    "\n",
    "\n",
    "    #all_features.append(tf.feature_column.numeric_column(col))\n",
    "#all_classes =  df.label\n",
    "#all_features \n",
    "#all_classes = all_classes.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericColumn(key='label', shape=(10,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=16, kernel_initializer = 'normal', activation='relu'))\n",
    "    model.add(Dense(16, kernel_initializer = 'normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer = 'normal', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarke\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarke\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FutureWarning: From version 0.22, errors during fit will result in a cross validation score of NaN by default. Use error_score='raise' if you want an exception raised or error_score=np.nan to adopt the behavior from version 0.22.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Equal' Op has type float32 that does not match type int32 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    512\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    978\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"metrics/acc/Cast_6:0\", shape=(?, 1), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-df2283ba73c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#cv_scores = cross_val_score(estimator,all_features,all_classes, cv=10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcv_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid shape for y: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m           **self.filter_sk_params(self.build_fn.__call__))\n\u001b[0;32m    155\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[0mloss_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-3b069526f7a3>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m       \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m           \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m           \u001b[0mskip_target_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_target_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           sample_weights=self.sample_weights)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# Prepare gradient updates and state updates.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[1;34m(self, outputs, skip_target_indices, targets, sample_weights, masks, return_stateful_result)\u001b[0m\n\u001b[0;32m   1842\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m                 \u001b[0moutput_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m                 return_stateful_result=return_stateful_result))\n\u001b[0m\u001b[0;32m   1845\u001b[0m         metric_results.extend(\n\u001b[0;32m   1846\u001b[0m             self._handle_per_output_metrics(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[1;34m(self, metrics_dict, y_true, y_pred, mask, weights, return_stateful_result)\u001b[0m\n\u001b[0;32m   1799\u001b[0m           \u001b[1;31m# stateless fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1800\u001b[0m           \u001b[0mstateful_metric_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstateful_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1801\u001b[1;33m           \u001b[0mmetric_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1802\u001b[0m           _track_metric_tensors(metric_name, metric_result,\n\u001b[0;32m   1803\u001b[0m                                 stateful_metric_result)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_call_stateless_fn\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m   1775\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_call_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m           \u001b[0mweighted_metric_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweighted_masked_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1777\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mweighted_metric_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_track_metric_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateless_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstateful_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[1;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[0;32m    645\u001b[0m     \"\"\"\n\u001b[0;32m    646\u001b[0m     \u001b[1;31m# score_array has ndim >= 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m     \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m       \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mbinary_accuracy\u001b[1;34m(y_true, y_pred, threshold)\u001b[0m\n\u001b[0;32m   1531\u001b[0m   \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1532\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1533\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   3307\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3308\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 3309\u001b[1;33m         \"Equal\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   3310\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3311\u001b[0m     result = _dispatch.dispatch(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    545\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 547\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Equal' Op has type float32 that does not match type int32 of argument 'x'."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "estimator = KerasClassifier(build_fn =create_model, epochs = 100, verbose = 0)\n",
    "#cv_scores = cross_val_score(estimator,all_features,all_classes, cv=10)\n",
    "cv_scores = cross_val_score(estimator,x,y, cv=10)\n",
    "cv_scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
